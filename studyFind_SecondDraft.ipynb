{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.8 from \"/Users/moweiting/Desktop/anaconda3/bin/python\"\n  * The NumPy version is: \"1.20.3\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: cannot import name '_set_madvise_hugepage' from 'numpy.core._multiarray_umath' (/Users/moweiting/Desktop/anaconda3/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-darwin.so)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/Desktop/anaconda3/lib/python3.8/site-packages/numpy/core/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmultiarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/anaconda3/lib/python3.8/site-packages/numpy/core/multiarray.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# _get_ndarray_c_version is semi-public, on purpose not added to __all__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m from ._multiarray_umath import (\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0m_fastCopyAndTranspose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_flagdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_insert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_vec_string\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_set_madvise_hugepage' from 'numpy.core._multiarray_umath' (/Users/moweiting/Desktop/anaconda3/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-darwin.so)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a2497aacdc73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasicConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%(asctime)s : %(levelname)s : %(message)s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/anaconda3/lib/python3.8/site-packages/gensim/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/anaconda3/lib/python3.8/site-packages/gensim/parsing/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m from .preprocessing import (remove_stopwords, strip_punctuation, strip_punctuation2,  # noqa:F401\n\u001b[0m\u001b[1;32m      5\u001b[0m                             \u001b[0mstrip_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_short\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_numeric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                             \u001b[0mstrip_non_alphanum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_multiple_whitespaces\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/anaconda3/lib/python3.8/site-packages/gensim/parsing/preprocessing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/anaconda3/lib/python3.8/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mheapq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/anaconda3/lib/python3.8/site-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/anaconda3/lib/python3.8/site-packages/numpy/core/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \"\"\" % (sys.version_info[0], sys.version_info[1], sys.executable,\n\u001b[1;32m     47\u001b[0m         __version__, exc)\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0menvkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menv_added\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.8 from \"/Users/moweiting/Desktop/anaconda3/bin/python\"\n  * The NumPy version is: \"1.20.3\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: cannot import name '_set_madvise_hugepage' from 'numpy.core._multiarray_umath' (/Users/moweiting/Desktop/anaconda3/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-darwin.so)\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import gensim \n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in data\n",
    "## Create long_des list, short_des list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open ('studies.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "long_des = list()\n",
    "short_des = list()\n",
    "for i in range (0,1000):\n",
    "    long_des.append(data[i]['longDescription'])\n",
    "    short_des.append(data[i]['shortDescription'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create data.txt which including all long_des as bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.txt\", 'w', encoding='utf8') as f:\n",
    "    for i in range (0, len(long_des)):\n",
    "        f.write(long_des[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train the model with short_des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create my_doc (short_des as input)\n",
    "## With gensim preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-14 13:34:19,871 : INFO : Done reading data file\n"
     ]
    }
   ],
   "source": [
    "def read_input(input_file):\n",
    "    for line in input_file:\n",
    "        yield gensim.utils.simple_preprocess (line)\n",
    "my_doc = list(read_input(short_des))\n",
    "logging.info (\"Done reading data file\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-14 13:34:23,561 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2021-08-14 13:34:23,564 : INFO : collecting all words and their counts\n",
      "2021-08-14 13:34:23,565 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-08-14 13:34:23,605 : INFO : collected 9246 word types from a corpus of 96928 raw words and 1000 sentences\n",
      "2021-08-14 13:34:23,606 : INFO : Loading a fresh vocabulary\n",
      "2021-08-14 13:34:23,630 : INFO : effective_min_count=2 retains 5310 unique words (57% of original 9246, drops 3936)\n",
      "2021-08-14 13:34:23,631 : INFO : effective_min_count=2 leaves 92992 word corpus (95% of original 96928, drops 3936)\n",
      "2021-08-14 13:34:23,653 : INFO : deleting the raw counts dictionary of 9246 items\n",
      "2021-08-14 13:34:23,654 : INFO : sample=0.001 downsamples 29 most-common words\n",
      "2021-08-14 13:34:23,655 : INFO : downsampling leaves estimated 70663 word corpus (76.0% of prior 92992)\n",
      "2021-08-14 13:34:23,675 : INFO : estimated required memory for 5310 words and 150 dimensions: 9027000 bytes\n",
      "2021-08-14 13:34:23,676 : INFO : resetting layer weights\n",
      "2021-08-14 13:34:25,146 : INFO : training model with 10 workers on 5310 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2021-08-14 13:34:25,189 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-08-14 13:34:25,222 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-08-14 13:34:25,243 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-08-14 13:34:25,250 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-08-14 13:34:25,251 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-08-14 13:34:25,255 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-08-14 13:34:25,262 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-08-14 13:34:25,264 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-08-14 13:34:25,264 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-08-14 13:34:25,265 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-08-14 13:34:25,266 : INFO : EPOCH - 1 : training on 96928 raw words (70669 effective words) took 0.1s, 626343 effective words/s\n",
      "2021-08-14 13:34:25,314 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-08-14 13:34:25,320 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-08-14 13:34:25,330 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-08-14 13:34:25,363 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-08-14 13:34:25,365 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-08-14 13:34:25,372 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-08-14 13:34:25,382 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-08-14 13:34:25,383 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-08-14 13:34:25,391 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-08-14 13:34:25,399 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-08-14 13:34:25,400 : INFO : EPOCH - 2 : training on 96928 raw words (70522 effective words) took 0.1s, 570773 effective words/s\n",
      "2021-08-14 13:34:25,444 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-08-14 13:34:25,485 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-08-14 13:34:25,506 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-08-14 13:34:25,508 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-08-14 13:34:25,509 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-08-14 13:34:25,511 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-08-14 13:34:25,514 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-08-14 13:34:25,517 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-08-14 13:34:25,521 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-08-14 13:34:25,524 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-08-14 13:34:25,528 : INFO : EPOCH - 3 : training on 96928 raw words (70574 effective words) took 0.1s, 590560 effective words/s\n",
      "2021-08-14 13:34:25,576 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-08-14 13:34:25,592 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-08-14 13:34:25,605 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-08-14 13:34:25,608 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-08-14 13:34:25,612 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-08-14 13:34:25,617 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-08-14 13:34:25,630 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-08-14 13:34:25,638 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-08-14 13:34:25,640 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-08-14 13:34:25,647 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-08-14 13:34:25,648 : INFO : EPOCH - 4 : training on 96928 raw words (70766 effective words) took 0.1s, 630199 effective words/s\n",
      "2021-08-14 13:34:25,694 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-08-14 13:34:25,743 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-08-14 13:34:25,747 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-08-14 13:34:25,749 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-08-14 13:34:25,751 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-08-14 13:34:25,752 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-08-14 13:34:25,754 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-08-14 13:34:25,755 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-08-14 13:34:25,757 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-08-14 13:34:25,759 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-08-14 13:34:25,762 : INFO : EPOCH - 5 : training on 96928 raw words (70640 effective words) took 0.1s, 672727 effective words/s\n",
      "2021-08-14 13:34:25,763 : INFO : training on a 484640 raw words (353171 effective words) took 0.6s, 573673 effective words/s\n",
      "2021-08-14 13:34:25,764 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2021-08-14 13:34:25,765 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2021-08-14 13:34:25,766 : INFO : training model with 10 workers on 5310 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2021-08-14 13:34:25,832 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-08-14 13:34:25,872 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-08-14 13:34:25,878 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-08-14 13:34:25,879 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-08-14 13:34:25,880 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-08-14 13:34:25,881 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-08-14 13:34:25,882 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-08-14 13:34:25,885 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-08-14 13:34:25,886 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-08-14 13:34:25,888 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-08-14 13:34:25,891 : INFO : EPOCH - 1 : training on 96928 raw words (70730 effective words) took 0.1s, 605179 effective words/s\n",
      "2021-08-14 13:34:25,932 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-08-14 13:34:26,008 : INFO : worker thread finished; awaiting finish of 8 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-14 13:34:26,013 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-08-14 13:34:26,014 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-08-14 13:34:26,015 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-08-14 13:34:26,016 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-08-14 13:34:26,019 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-08-14 13:34:26,021 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-08-14 13:34:26,022 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-08-14 13:34:26,025 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-08-14 13:34:26,029 : INFO : EPOCH - 2 : training on 96928 raw words (70630 effective words) took 0.1s, 572383 effective words/s\n",
      "2021-08-14 13:34:26,083 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-08-14 13:34:26,131 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-08-14 13:34:26,142 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-08-14 13:34:26,144 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-08-14 13:34:26,146 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-08-14 13:34:26,147 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-08-14 13:34:26,149 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-08-14 13:34:26,150 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-08-14 13:34:26,151 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-08-14 13:34:26,152 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-08-14 13:34:26,155 : INFO : EPOCH - 3 : training on 96928 raw words (70711 effective words) took 0.1s, 611190 effective words/s\n",
      "2021-08-14 13:34:26,205 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-08-14 13:34:26,258 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-08-14 13:34:26,264 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-08-14 13:34:26,266 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-08-14 13:34:26,268 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-08-14 13:34:26,269 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-08-14 13:34:26,271 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-08-14 13:34:26,274 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-08-14 13:34:26,276 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-08-14 13:34:26,277 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-08-14 13:34:26,278 : INFO : EPOCH - 4 : training on 96928 raw words (70710 effective words) took 0.1s, 617314 effective words/s\n",
      "2021-08-14 13:34:26,327 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-08-14 13:34:26,392 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-08-14 13:34:26,394 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-08-14 13:34:26,395 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-08-14 13:34:26,399 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-08-14 13:34:26,400 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-08-14 13:34:26,401 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-08-14 13:34:26,403 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-08-14 13:34:26,404 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-08-14 13:34:26,406 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-08-14 13:34:26,408 : INFO : EPOCH - 5 : training on 96928 raw words (70609 effective words) took 0.1s, 593418 effective words/s\n",
      "2021-08-14 13:34:26,459 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-08-14 13:34:26,523 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-08-14 13:34:26,525 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-08-14 13:34:26,530 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-08-14 13:34:26,532 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-08-14 13:34:26,534 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-08-14 13:34:26,536 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-08-14 13:34:26,538 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-08-14 13:34:26,540 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-08-14 13:34:26,543 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-08-14 13:34:26,545 : INFO : EPOCH - 6 : training on 96928 raw words (70549 effective words) took 0.1s, 548546 effective words/s\n",
      "2021-08-14 13:34:26,588 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-08-14 13:34:26,641 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-08-14 13:34:26,654 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-08-14 13:34:26,656 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-08-14 13:34:26,658 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-08-14 13:34:26,659 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-08-14 13:34:26,662 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-08-14 13:34:26,663 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-08-14 13:34:26,665 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-08-14 13:34:26,667 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-08-14 13:34:26,667 : INFO : EPOCH - 7 : training on 96928 raw words (70748 effective words) took 0.1s, 613704 effective words/s\n",
      "2021-08-14 13:34:26,725 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-08-14 13:34:26,770 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-08-14 13:34:26,786 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-08-14 13:34:26,793 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-08-14 13:34:26,796 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-08-14 13:34:26,798 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-08-14 13:34:26,799 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-08-14 13:34:26,802 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-08-14 13:34:26,804 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-08-14 13:34:26,806 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-08-14 13:34:26,808 : INFO : EPOCH - 8 : training on 96928 raw words (70700 effective words) took 0.1s, 572644 effective words/s\n",
      "2021-08-14 13:34:26,866 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-08-14 13:34:26,871 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-08-14 13:34:26,917 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-08-14 13:34:26,924 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-08-14 13:34:26,933 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-08-14 13:34:26,942 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-08-14 13:34:26,944 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-08-14 13:34:26,954 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-08-14 13:34:26,955 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-08-14 13:34:26,960 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-08-14 13:34:26,962 : INFO : EPOCH - 9 : training on 96928 raw words (70644 effective words) took 0.1s, 512738 effective words/s\n",
      "2021-08-14 13:34:27,004 : INFO : worker thread finished; awaiting finish of 9 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-14 13:34:27,055 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-08-14 13:34:27,064 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-08-14 13:34:27,066 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-08-14 13:34:27,067 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-08-14 13:34:27,068 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-08-14 13:34:27,070 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-08-14 13:34:27,071 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-08-14 13:34:27,073 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-08-14 13:34:27,076 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-08-14 13:34:27,077 : INFO : EPOCH - 10 : training on 96928 raw words (70733 effective words) took 0.1s, 649219 effective words/s\n",
      "2021-08-14 13:34:27,079 : INFO : training on a 969280 raw words (706764 effective words) took 1.3s, 539022 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(706764, 969280)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec (my_doc, size=150, window=10, min_count=2, workers=10)\n",
    "model.train(my_doc,total_examples=len(my_doc),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-14 13:34:29,060 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"word 'policy' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7e4c6984ac9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mw1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"policy\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/anaconda3/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/anaconda3/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'policy' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "w1 = \"policy\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'dirty' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ea067b34c2ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mw2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"dirty\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/anaconda3/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/anaconda3/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'dirty' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "w2 = \"dirty\"\n",
    "model.wv.most_similar(positive = w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train the model with online corpus via gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-21 13:06:29,452 : INFO : loading projection weights from /Users/moweiting/gensim-data/glove-twitter-25/glove-twitter-25.gz\n",
      "2021-07-21 13:07:21,082 : INFO : loaded (1193514, 25) matrix from /Users/moweiting/gensim-data/glove-twitter-25/glove-twitter-25.gz\n"
     ]
    }
   ],
   "source": [
    "model_glove_twitter = api.load(\"glove-twitter-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-83-0dc60c9c1bef>:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  my_similar_word = model_glove_twitter.wv.most_similar(\"social\",topn=10)\n",
      "2021-07-21 13:07:24,806 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('marketing', 0.9278293251991272),\n",
       " ('popular', 0.9230425357818604),\n",
       " ('personal', 0.9197487831115723),\n",
       " ('media', 0.9193164110183716),\n",
       " ('network', 0.8823835849761963),\n",
       " ('web', 0.8817125558853149),\n",
       " ('local', 0.8731585741043091),\n",
       " ('internet', 0.8709415197372437),\n",
       " ('business', 0.8625822067260742),\n",
       " ('civil', 0.8574709296226501)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_similar_word = model_glove_twitter.wv.most_similar(\"social\",topn=10)\n",
    "my_similar_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'marketing'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_similar_word[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make everything into Stem version (everything in my corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "short_des_stem = []\n",
    "for i in range (len(short_des)):\n",
    "    short_des_stem.append(list())\n",
    "    nltk_tokens = nltk.word_tokenize(short_des[i])\n",
    "    for w in nltk_tokens:\n",
    "        short_des_stem[i].append(porter_stemmer.stem(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compare stem version \"similar words\" with stem version corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inflammatory bowel disease IBD, including Crohn's disease (CD) and ulcerative colitis (UC), is a chronic inflammatory disease of the intestinal tract with unknown etiology. The mechanism includes environment, genetics, intestinal microecology and immunity. In recent years, the incidence of IBD in Asian countries has continued to rise, and the incidence of CD and UC in Guangzhou has reached 1.09/10 million and 2.05/10 million respectively. The patient suffers from the disease for a long time, which greatly affects the mentality and the quality of life. However, the mentality and quality of life of IBD patients have not received the attention they deserve, and research in this area is relatively lacking. The study by Lewis et al. found that among IBD patients, up to 1/3 of depression patients and 2/3 of anxiety patients were not diagnosed in time. This study conducted an Internet questionnaire survey on IBD patients diagnosed in the Department of Gastroenterology, the Second Affiliated Hospital of Zhejiang University School of Medicine, to learn about the patients 'mental and psychological state, to screen the risk factors of patients'mental and psychological diseases, and to understand the impact of mental and psychological factors on the quality of life and quality of IBD patients. The impact of the disease provides a scientific basis for the optimal treatment of IBD.\n",
      "------------------\n",
      "Background: In patients with acute ST-elevation myocardial infarction (STEMI), the amount of infarcted myocardium (infarct size) is known to be a major predictor for adverse remodeling and recurrent adverse cardiovascular events. Effective cardio-protective strategies with the aim of reducing infarct size are therefore of great interest. Local and systemic inflammation influences the fate of ischemic myocardium and thus, adverse remodeling and clinical outcome. C-reactive protein (CRP) also acts as a potential mechanistic mediator that adversely affects the amount of irreversible myocardial tissue damage after acute myocardial infarction.\n",
      "Objective: The main objectives of the current study are to investigate the efficacy of selective CRP apheresis, using the PentraSorb®-CRP system, as an adjunctive therapy to standard of care for patients with acute STEMI treated with primary PCI.\n",
      "Design: Investigator-initiated, prospective, randomized, open-label (outcome assessors masked), controlled, multicenter, two group trial with a two-stage adaptive design.\n",
      "Innovation: Selective CRP apheresis offers potential to decrease infarct size and consequently improve outcome after PCI for STEMI. This is the first randomized trial investigating the impact of selective CRP apheresis on infarct size in post-STEMI patients. In perspective, the study design allows furthermore to collect robust evidence for the design of a definitive outcome study.\n",
      "------------------\n",
      "This study integrates the Mental Health Research Network (MHRN) suicide risk models into Opioid Wizard, an electronic health record (EHR) clinical decision support (CDS) to identify and treat patients at high risk of opioid use disorder (OUD)/overdose or diagnosed with OUD, to alert primary care clinicians (PCCs) to patients at elevated risk for suicide and guide them through structured suicide risk assessment. In both intervention and control clinics, suicide risk scores will be calculated for all Opioid Wizard-eligible patients and relevant EHR data to inform analyses will be archived. In intervention clinics, Opioid Wizard will alert PCCs to Opioid Wizard-eligible patients who are at increased risk of suicide and coach them through use of the Columbia Suicide Severity Risk Scale (CSSRS), a structured tool in the EHR that will help PCCs assess immediate suicide risk. Based on the resulting CSSRS score, Opioid Wizard will provide EHR links for risk-based referrals and follow-up recommendations, including care as usual, routine or emergent referral to behavioral health, or transportation to the emergency department (ED) for further assessment. Primary outcome measures include completion of CSSRS assessments for at-risk patients and patient engagement in outpatient mental health care.\n",
      "------------------\n",
      "The purpose of this study is to evaluate the safety, and tolerability of ASP0739, when administered as a single agent and in combination with pembrolizumab.\n",
      "This study will also evaluate the clinical response and other measures of anticancer activity of ASP0739 when administered as a single agent and in combination with pembrolizumab based on central and local assessment.\n",
      "------------------\n",
      "Targeting the PD-L1 pathway with atezolizumab has demonstrated objective responses across a broad range of malignancies including head and neck squamous cell carcinoma (SCCHN). MO39839 is a window of opportunity study investigating the feasibility, safety and postoperative complication rates of preoperative short time immunotherapy with atezolizumab in patients with local SCCHN. In the scope of MO39839 a comprehensive translational research program will be conducted to assess the potential effect of atezolizumab on dynamics in tumor immunity, and to identify and validate potential predictive and prognostic biomarkers.\n",
      "------------------\n",
      "The investigators aim to use a repeated measures observational study utilising a battery of multimodal assessment tools (symptom, cognitive, visual, motor). The investigators aim to recruit 200 rugby players (male and female) from University Rugby Union teams and local amateur rugby clubs in the North East of England. The multimodal battery assessment used in this study will compare metrics between digital methods and against traditional assessment.\n",
      "------------------\n",
      "Alcohol use and its consequences represent an important public health problem. As well as alcohol dependence, hazardous drinking also contributes to a high burden in terms of morbidity and mortality. To improve these patients' prognosis and decrease associated social and health care costs, it is necessary to increase early detection, intervention and treatment for these problems. Alcohol consumption is associated with a decrease in primary care services utilization, thus Emergency Departments (EDs) are a primary gateway to healthcare services in this group.\n",
      "Depending on the investigative method and the mixture of the target population, an estimated 0.6-40% of all ED visits are due to alcohol-related problems. Given this, EDs offer a unique window of opportunity to address alcohol problems.\n",
      "The threshold most commonly used to define frequent use of EDs is more than 4 visits per year. Frequent users comprise 0.3% to 10% of all ED patients and account for 3.5% to 28% of ED visits in developed countries. Addictive and other psychiatric disorders, and also social vulnerability are more common in frequent ED users than in non-frequent users. Although case management interventions seem promising to reduce ED attendance among frequent users, currently there is mixed evidence on the effects of such interventions on ED use.\n",
      "Considering all this, a broader understanding of interventions to reduce frequent visits is needed, specially focusing on local frequent ED populations and identified highly vulnerable subgroups, such as hazardous drinkers.\n",
      "The investigators aim to evaluate the feasibility and potential effectiveness of a Case Management programme for ED Frequent Users presenting risky alcohol use in the ED of a tertiary hospital.\n",
      "------------------\n",
      "This is a retrospective, observational, multicenter study to collect Real-World Evidence (RWE) data on systemic AL-AMY patients in Europe. Data from paper/electronic medical records and/or electronic databases from key reference centers in Europe will be used. Data will either be entered by the site staff in the electronic Case Report Form (eCRF) or, where feasible, transferred directly, always in accordance to local regulations.\n",
      "------------------\n",
      "A majority of residents in low income communities have been exposed to a potentially traumatic event, and up to half (30-50%) of trauma-exposed residents in safety net clinical settings meet criteria for posttraumatic stress disorder (PTSD). Despite this, only 13% receive treatment. Poor access to PTSD treatment is due to a shortage of mental health specialists.\n",
      "This study aims to evaluate the implementation and effectiveness of a brief, cognitive-behavioral intervention for posttraumatic stress disorder (PTSD)-Skills Training in Affective and Interpersonal Regulation (STAIR)- that will be offered in Boston Medical Center (BMC)'s primary care clinics as the new standard of care following integrated behavioral healthcare (IBH) therapist training. In response to clinician capacity concerns and the impact of the COVID-19 pandemic, we will be offering the intervention in both clinician-administered and self-paced, web-administered formats. The evidence base suggests that STAIR, delivered both synchronously (in-person/telehealth STAIR) and asynchronously (webSTAIR), is associated with significant improvements in PTSD and depression symptoms.\n",
      "------------------\n",
      "The research team has designed, and will test, a web tool to support traditional Maternal Education (EM). This tool is organized into 4 areas: 1) Information área; 2) Communication área (with peers and with professionals) 3) Self-management health area(instruments to check or reflect on their own health needs, as well as decision algorithms to detect the time of delivery and postpartum alarm symptoms); 4) Clinical data área (the woman can have her clinical data, include it and share it with other professionals) The objective of the study is to evaluate the clinical effectiveness of the EMAeHealth tool and also its implementation in the real world (its usability and acceptability, by women and professionals).\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "len(my_doc)\n",
    "count = [0]*len(my_doc)\n",
    "for i in range (len(my_doc)):\n",
    "    for j in range (len(my_similar_word)):\n",
    "        if my_similar_word[j][0] in my_doc[i]:\n",
    "            count[i] = count[i] + 1\n",
    "\n",
    "my_result = sorted(range(len(count)), reverse = True,key=lambda k: count[k])[:10]\n",
    "for j in range (len(my_result)):\n",
    "    print(short_des[my_result[j]])\n",
    "    print(\"------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find similar phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.wrappers import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model trained by data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "# Skipgram model :\n",
    "model = fasttext.train_unsupervised(\"data.txt\".encode(), model='skipgram')\n",
    "\n",
    "# or, cbow model :\n",
    "model = fasttext.train_unsupervised('data.txt', model='cbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00826498 -0.03562757  0.06132699 -0.06196772 -0.11318931  0.05985509\n",
      "  0.00063149 -0.22486368 -0.06414268  0.40337154  0.25087008 -0.18738896\n",
      "  0.11867779  0.04273482  0.0164328   0.122447    0.00843596  0.05657418\n",
      " -0.11740816 -0.04559429  0.5543389  -0.09899044 -0.10574005 -0.04928178\n",
      "  0.09750175  0.1588159   0.08704684  0.25427082 -0.20817617  0.01756314\n",
      "  0.10178141  0.09947927 -0.08412265 -0.20653895 -0.15445057  0.13438213\n",
      " -0.00718551 -0.2162596   0.06123563 -0.25663787  0.19720882 -0.07130568\n",
      " -0.149208    0.11382615  0.5532628   0.07966141  0.05450255  0.03324973\n",
      " -0.08610424 -0.12121249 -0.03587046  0.39894953 -0.3562577   0.14347477\n",
      " -0.14986913  0.06094816 -0.33283886 -0.13934293 -0.1659927   0.1469585\n",
      "  0.13793409  0.10626914  0.12905437  0.33052805 -0.07804996  0.15017937\n",
      " -0.4132499   0.10992378  0.02965744 -0.22203021 -0.0208174   0.35350505\n",
      "  0.0217065  -0.04518151  0.23514609  0.15778823 -0.08866913  0.11044579\n",
      " -0.41666847  0.4590445   0.2960946  -0.15523423 -0.03726768 -0.22436696\n",
      " -0.06397442  0.0910743   0.1778866   0.2123861   0.0617805  -0.0493442\n",
      "  0.14618579  0.32964125  0.0401894   0.07744513 -0.08851196 -0.03342599\n",
      " -0.231311   -0.13630123 -0.09028183 -0.13416637]\n",
      "[-0.0044382  -0.02999078  0.05151184 -0.05169078 -0.09427344  0.04918981\n",
      "  0.00166545 -0.18511185 -0.052974    0.3352587   0.2083755  -0.15638821\n",
      "  0.09641612  0.03442671  0.01424425  0.10162429  0.00677599  0.04709867\n",
      " -0.09800868 -0.03833063  0.45944166 -0.08161659 -0.08638094 -0.04025045\n",
      "  0.08196814  0.13213485  0.0739207   0.21082984 -0.17228074  0.01406418\n",
      "  0.0840509   0.08347173 -0.06988636 -0.17089553 -0.12688914  0.11003833\n",
      " -0.00487567 -0.17958775  0.04881291 -0.21212415  0.164425   -0.06000448\n",
      " -0.12377966  0.09494055  0.45757106  0.06779916  0.04635009  0.02689444\n",
      " -0.07037795 -0.1007189  -0.02852031  0.32806897 -0.29592115  0.11825503\n",
      " -0.12359778  0.05117533 -0.274089   -0.11390625 -0.1388776   0.12304877\n",
      "  0.11551023  0.0880638   0.10709003  0.27389738 -0.06671158  0.12532274\n",
      " -0.34219235  0.08992585  0.02482522 -0.18483044 -0.01519162  0.29141402\n",
      "  0.01644659 -0.0350014   0.19599676  0.1300303  -0.07136161  0.08827846\n",
      " -0.34416047  0.37919247  0.24433805 -0.12874305 -0.03111401 -0.18591423\n",
      " -0.05133447  0.07477985  0.14688341  0.17759196  0.05105897 -0.04218764\n",
      "  0.12092985  0.27103782  0.03519558  0.06400383 -0.07205807 -0.02820592\n",
      " -0.19223328 -0.1123051  -0.07308477 -0.1101696 ]\n"
     ]
    }
   ],
   "source": [
    "#print(model.words)   # list of words in dictionary\n",
    "print(model['sunshine']) # get the vector of the word 'king'\n",
    "print(model['sun shine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9999807476997375, 'bupivacaine'),\n",
       " (0.9999790191650391, 'lidocaine'),\n",
       " (0.9999747276306152, 'nalbuphine'),\n",
       " (0.9999727010726929, 'ropivacaine'),\n",
       " (0.9999669194221497, 'phenylephrine'),\n",
       " (0.9999666810035706, 'Ketamine'),\n",
       " (0.9999574422836304, 'B-alanine'),\n",
       " (0.9999566078186035, 'June'),\n",
       " (0.99995356798172, 'machine'),\n",
       " (0.9999451041221619, 'bolus')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_nearest_neighbors('sun shine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model trained by online corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-08-13 15:06:21--  https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2\n",
      "Resolving dumps.wikimedia.org... 208.80.154.7\n",
      "Connecting to dumps.wikimedia.org|208.80.154.7|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 18854663069 (18G) [application/octet-stream]\n",
      "Saving to: 'enwiki-latest-pages-articles.xml.bz2.1'\n",
      "\n",
      "enwiki-latest-pages 100%[===================>]  17.56G  4.76MB/s    in 63m 7s  \n",
      "\n",
      "2021-08-13 16:09:28 (4.75 MB/s) - 'enwiki-latest-pages-articles.xml.bz2.1' saved [18854663069/18854663069]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-08-13 16:10:53--  http://mattmahoney.net/dc/enwik9.zip\n",
      "Resolving mattmahoney.net... 67.195.197.24\n",
      "Connecting to mattmahoney.net|67.195.197.24|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 322592222 (308M) [application/zip]\n",
      "Saving to: 'data/enwik9.zip'\n",
      "\n",
      "enwik9.zip          100%[===================>] 307.65M  1.34MB/s    in 3m 37s  \n",
      "\n",
      "2021-08-13 16:14:32 (1.42 MB/s) - 'data/enwik9.zip' saved [322592222/322592222]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -c http://mattmahoney.net/dc/enwik9.zip -P data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data/enwik9.zip\n",
      "  inflating: data/enwik9             \n"
     ]
    }
   ],
   "source": [
    "!unzip data/enwik9.zip -d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "!perl wikifil.pl data/enwik9 > data/fil9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anarchism originated as a term of abuse first used against early working class "
     ]
    }
   ],
   "source": [
    "!head -c 80 data/fil9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_unsupervised('data/fil9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"data/fil9.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "model = fasttext.load_model(\"data/fil9.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.get_nearest_neighbors('medical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split 连着的 phrase into seperate words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['paramed'],\n",
       " ['non', 'medic'],\n",
       " ['pre', 'hospit'],\n",
       " ['medicin'],\n",
       " ['dentistri'],\n",
       " ['veterinari'],\n",
       " ['pae', 'di', 'at', 'ric', 's'],\n",
       " ['physician'],\n",
       " ['clinic'],\n",
       " ['healthcar']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_similar_word = []\n",
    "my_similar_word_stem = []\n",
    "for i in range (len(results)):\n",
    "    my_similar_word.append(wordninja.split(results[i][1]))\n",
    "for j in range (len(my_similar_word)):\n",
    "    my_similar_word_stem.append(list())\n",
    "    for w in my_similar_word[j]:\n",
    "        my_similar_word_stem[j].append(porter_stemmer.stem(w))\n",
    "my_similar_word_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find similar words/ phrases in my corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_doc)\n",
    "count = [0]*len(short_des_stem)\n",
    "for i in range (len(short_des_stem)):\n",
    "    for j in range (len(my_similar_word)):\n",
    "        for w in my_similar_word[j]:\n",
    "            if porter_stemmer.stem(w) in short_des_stem[i]:\n",
    "                count[i] = count[i] + 1\n",
    "\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[400, 460, 49, 716, 724, 733, 904, 913, 980, 11]\n",
      "During the peak of the second COVID -19 wave, the hospitals were over-crowded. Many COVID -19 positive patients had to stay at home and reach out to their family physicians for guidance. Medical follow-up for these patients was a daunting challenge. As in - patient hospital facilities were not readily accessible due to over crowding, early objective tests to identify home quarantined patients prone to deterioration and timely medical intervention to avoid hospitalization were required.\n",
      "Based on early assessment of inflammatory markers like CRP and clinical signs like persistent high-grade fever, need-based early medical intervention was initiated in home quarantined COVID -19 patients prior to the onset of hypoxia, in order to avoid complications and hospitalization\n",
      "------------------\n",
      "Abstract Purpose: To study the long-term effectiveness of case-management rehabilitation intervention among patients after myocardial infarction (MI) compared with the current standard of care.\n",
      "Methods: Participants were 151 patients who underwent uncomplicated MI and of which nearly all enrolled in a cardiac rehabilitation program. Patients were randomized into an intervention or control group and provided two years of follow-up data. The intervention, conducted within an occupational medicine clinic, started during hospitalization or immediately thereafter and continued for 2 years. It included: early referral to an occupational physician, charting an occupational intervention program, coordinating between the patient and relevant parties, psychosocial intervention, intensive follow-up sessions during the first 1.5 months, and more spaced interventions during the follow-up period. Outcome variables were: return to work within 6 months of hospitalization and maintenance of employment at one and two years of follow-up.\n",
      "------------------\n",
      "This study will compare two different ways of giving cagrilintide and semaglutide for treating overweight and obesity. The medicines will either be given together in 1 injection or as 2 separate injections.\n",
      "The aim of the study is to find out how the different ways of injection affect the level of the medicines in the blood.\n",
      "For the first 14 weeks of the study, participants will get cagrilintide and semaglutide as 2 separate injections. Then participants will either switch to getting the medicines as a combined injection or continue to get the separate injections for 8 weeks. Which treatment participants get after the first 14 weeks is decided by chance.\n",
      "Participants will get the study medicines once a week for 22 weeks. A study nurse at the clinic will inject the medicines with a thin needle in participants stomach area.\n",
      "The study will last for about 8 months.Participants will have 28 clinic visits with the study staff. For 4 of these visits, participants will stay in the clinic for 5 nights.\n",
      "Participants will have blood drawn at 21 visits. Participants will have clinical assessments and participants will be asked about their health, medical history and habits including mental health questionnaires.\n",
      "For women: Participants must not be able to become pregnant if they wish to participate in this study.\n",
      "------------------\n",
      "Improved cancer survival has led to increased attention on long-term health and quality of life (QoL) among the survivors. Both the cancer diagnosis and intensive treatments increase the risk of late effects which may interfere with daily physical, psychological and social functioning, and thereby negatively affect their QoL. Well-documented late-effects among cancer survivors are second cancer, cardio-vascular disease, pain, hormone disturbances, mental distress and chronic fatigue (CF).\n",
      "CF is a subjective experience of substantial lack of energy, exhaustion and cognitive difficulties lasting for six months or longer. CF is one of the most common and distressing late effects after cancer, affecting 15-35 % of survivors, often for years beyond treatment. Despite the high prevalence and the huge negative consequences of CF on daily functioning and QoL and the economic and societal costs, effective treatment of CF and standardized follow-up care are currently lacking.\n",
      "CF is a complex condition best understood as a multifactorial phenomenon. Our and other research groups have examined various cohorts of cancer survivors in order to identify behavioral-, psychological-, and biological factors associated with CF, that can form the basis for targeted interventions. So far, few treatable biological factors have been identified, even though immune activation, flattened diurnal cortisol slopes and a blunted cortisol response to stress have been demonstrated in small studies among cancer survivors suffering from CF. On the other hand, several modifiable behavioral factors including emotional distress, physical inactivity, sleep disturbances and unhealthy diets are found to be associated with CF. So far, most of the interventions aiming to reduce fatigue during and shortly after cancer treatment have targeted only one of these factors at a time, with small to moderate effect sizes. No prior study has examined if CF in cancer survivors is better treated by a complex intervention targeting combinations of these factors, an approach which seems logical due to the complexity of the symptom.\n",
      "The Division of Cancer Medicine at Oslo University Hospital (OUH) presently offers limited rehabilitation programs, including patient education, physical exercise, cognitive behavioral program and nutrition counselling to cancer survivors with CF. However, these programs are not offered as an interdisciplinary intervention integrated in a standardized patient care pathway, and the effects of these interventions have not been assessed. Based on the investigators clinical experience and published studies on single-targeted interventions, the investigators hypothesize that a complex intervention including psycho-educational elements, physical exercise and nutrition counseling delivered as a standardized patient care pathway is well-founded and doable, and will improve fatigue, functioning and QoL in cancer survivors with CF.\n",
      "During the fall of 2021, the investigators will conduct a randomized controlled trial (RCT) with the overall objective to improve fatigue in lymphoma survivors with CF. To uncover strengths and weaknesses with the planned RCT, i.e. the inclusion procedures, the assessments and the complex intervention, the investigators are now conducting a small one-armed feasibility study before the RCT during spring 2021.\n",
      "------------------\n",
      "Gait disturbances and movement restrictions occur frequently in Parkinson's disease. Patient-centered monitoring with objective aids in the patient's daily life, supports and promotes therapy decisions made by physicians and patients. Technical, sensor-based monitoring has the potential to generate objective target parameters at any point in time during therapy (patient journey), representing the state of health and its progression, and to make this information available to physicians and patients via telemedical data management. In this study, the gait analysis system \"Mobile GaitLab Home 2.0\", consisting of sensors for gait data acquisition, a smartphone application for study participants (Mobile GaitLab app) and a web portal for physicians (Mobile GaitLab portal) is used for data collection.\n",
      "The research question is divided into three sub-objectives: First, the study explores and tests how technically generated parameters of sensor-based gait analysis can map the symptom \"bradykinesis\". The second goal is the explorative investigation of how a tele-health service support with low-threshold access to medical professionals, can be integrated into the care process. The third goal is the implementation evaluation of the technological developments. Here, it is examined to determine the extent to which the implementation of gait data and patient feedback (PROMs) in the patient-centered care process within the framework of clinical decision support contributes to early gait-associated therapy optimization and thus improves the general health of patients and how initial indications of positive care effects for patients can be derived.\n",
      "During a 60-day observation phase, study participants use the gait analysis system, which records their gait pattern throughout the day and collects data via the Mobile GaitLab app. Study participants are asked to perform standardized gait tests in the home environment several times a day, in addition to continuous measurements during the awake phase. Frequency of data collection is controlled by Mobile GaitLab Home 2.0 and can be flexibly adjusted to the study participant's health status and therapy. The Mobile GaitLab app uses questionnaires to record data on gait safety, activity, general well-being, and events relevant to the disease. An evaluation of these data (PROMs) and the results from the gait analyses, are visualized for the study participants via the Mobile GaitLab app.\n",
      "------------------\n",
      "1.1 Background Obstructions of the lacrimal drainage system can be differentiated based on anatomical location or severity. But also, differentiating between congenital and acquired nasolacrimal duct obstruction (NLDO) is possible. The incidence of congenital NLDO (CNLOD) has been shown to be approximately 20%. Most of the cases undergo spontaneous remission, as the ductus nasolacrimalis may open spontaneously. About 2-12% display a symptomatic course. 2. S: 290; 3; 4, 5 Acquired NLDO may occur during childhood and adulthood. The incidence of symptomatic acquired NLDO is around 30 cases per 100.000 people in an US-based cohort study. S: 293; 30 Two major anatomical closure sites have been described, which are on the one hand located at the between the punctum and canaliculus, and on the other hand located after the lacrimal sac. 1. S. ; 2. S: 293 1.2 Aim of this study The aim of this study is to assess the success rates for different types of primary tear duct surgery, performed from 2013 - 2017 at the department of Ophthalmology and Optometry, Medical University of Vienna.\n",
      "Treatment success was defined as the absence of clinical signs of lacrimal drainage system obstruction (epiphora, increased tear leak, mucous discharge) and without the need for re-intervention.\n",
      "It is further investigated whether the type of operation performed or whether the silicone tube used influence success rates.\n",
      "1.3 Methods A retrospective chart analysis of all patients - independent of the underlying pathology - undergoing surgery of the lacrimal drainage system between 1st of January 2013 and 31st December 2017. Success rates and patient profiles will be analyzed not only for all patients, but also for subgroups based on the underlying pathology and operation performed.\n",
      "------------------\n",
      "Background: Immunosenescence is an aging-dependent phenomenon underlying age dependent deterioration in the function of the immune system, characterized by a decline in B and T cells with a relative increase in natural killer (NK) cells. Aging also promotes chronic inflammation accompanied by increased levels of pro-inflammatory cytokines. Both immunosenescence and inflammation contribute to frailty, which is a geriatric syndrome characterized by age-related deterioration in multiple physiological systems resulting in greater vulnerability to stressors and increased risk of poor outcomes including longer hospital stays, postoperative complications, poor responses to vaccination, functional decline, and death.\n",
      "Although pharmacological interventions could be developed to address immunosenescence, inflammation and frailty, a dietary intervention that does not cause weight or muscle loss may be a preferable option, particularly if it is periodic in nature and it only needs to be adopted for a few weeks per year.\n",
      "Hypothesis: We will test the hypothesis that a newly formulated and relatively high calorie fasting mimicking diet (FMD) to be administered to subjects age 65-80 once a month for 5 days for two to six cycles can partially reverse immunosenescence and inflammation, thus contributing to the reduction of frailty.\n",
      "Aims: This proposal is divided into 2 main tasks:\n",
      "Task 1: We will determine whether FMD cycles in mice: a) prevent frailty syndrome onset and symptoms B) delay or reverse age-related immunosenescence and inflammaging, C) improve the functionality of bone marrow cells, D) enhances the response to flu vaccination. Task 2: A )We will develop a special relatively high calorie FMD medical food for testing in humans, B) We will test the safety and efficacy of the FMD medical food in an aged and frail individuals (65-80 yr) for 2-5 day cycles preceding their annual influenza vaccination. Expected results: In mice, we expect that the FMD diet will reduce the clinical signs of frailty during aging, and in particular increase immune system influenza vaccine response by preventing immunesenescence. We expect that the FMD will reduce phosphorylation of mTOR and of its downstream targets, and induce autophagy and apoptosis in WBCs. These effects are anticipated to remove damaged cells and promote the activation of hematopoietic stem cells and the generation of new WBCs. We also expect that the transient increase in corticosteroids and removal of damage immune cells will be accompanied by a decrease in systemic inflammation. Increased performance on rotarod and other measures of frailty is also anticipated. In humans, we expect that the FMD will be well tolerated by the pre-frail elderly without major adverse events and that it will be possible to achieve high compliance to this diet. We also anticipate that elderly undergoing the FMD protocol followed by 30 days of a normal diet plus supplements will exhibit better functional status and better response to the flu vaccine as compared to patients from the control arm. An improvement in handgrip strength and in lean body mass, as detected by BIA, is also expected, at least in a fraction of the patients from the intervention arm. Impact: Frailty is a geriatric syndrome characterized by age-related deterioration in multiple physiological systems and homeostatic mechanisms, resulting in greater vulnerability to stressors and increased risk of poor outcomes including longer hospital stays, postoperative complications, poor responses to vaccination, functional decline, and death. Thus, the identification of a dietary strategy, potentially to be applied for only 10 days a year but able to rejuvenate the immune profile and function while reducing systemic inflammation could have a major impact on both healthspan and health-related expenses. Because older individuals are often taking multiple drugs, the dietary intervention being investigated here would also reduce the potential toxicity of an additional pharmacological intervention.\n",
      "------------------\n",
      "Cancer is a condition where cells in a specific part of body grow and reproduce uncontrollably. Non-small cell lung cancer (NSCLC) is a solid tumor, a disease in which cancer cells form in the tissues of the lung. The purpose of this study is to determine if telisotuzumab vedotin works better than docetaxel and to assess how safe telisotuzumab vedotin is in adult participants with NSCLC who have previously been treated. Change in disease activity and adverse events will be assessed.\n",
      "Telisotuzumab vedotin is an investigational drug being developed for the treatment of NSCLC. Study doctors put the participants in 1 of 2 groups, called treatment arms. Each group receives intravenous (IV) infusion of telisotuzumab vedotin or IV infusion of docetaxel. Approximately 600 adult participants with c-Met+ NSCLC will be enrolled in the study in approximately 250 sites worldwide.\n",
      "Participants will receive IV telisotuzumab vedotin every 2 weeks or docetaxel every 3 weeks until meeting study drug discontinuation criteria.\n",
      "There may be higher treatment burden for participants in this trial compared to their standard of care. Participants will attend regular visits during the study at a hospital or clinic. The effect of the treatment will be checked by medical assessments, blood tests, checking for side effects and completing questionnaires.\n",
      "------------------\n",
      "Vitiligo is a common chronic autoimmune disease that causes the body's immune system to attack its own pigment producing skin cells. This study is to evaluate how safe and effective upadacitinib is in participants with non-segmental vitiligo. Adverse effects and change in disease activity will be assessed.\n",
      "Upadacitinib is being evaluated for the treatment of non-segmental vitiligo. The study will enroll approximately 160 participants aged 18-65 with non-segmental vitiligo in 5 treatment arms across 35 sites worldwide.\n",
      "Participants will either receive study drug vs placebo oral tablets once daily (QD) for 24 weeks (Period A). In Period B (up to 52 weeks), participants who received placebo during the first 24 weeks will switch to study drug. Participants who received study drug during the first 24 weeks, will continue to receive study drug.\n",
      "There may be higher treatment burden for participants in this trial compared to their standard of care. Participants will attend regular visits during the study at a hospital or clinic. The effect of the treatment will be checked by medical assessments, blood tests, checking for side effects and completing questionnaires.\n",
      "------------------\n",
      "Open label, phase II study non randomized single group assignment of 20 evaluable patients 13 years and older, over 37,5 kg body-weight, with sensorineural hearing loss of at least 20 dB at 8 kHz in high frequency average (HFA), and with documented genetic mutations in the WFS1 gene and with at least one other major documented clinical symptom pertaining to Wolfram syndrome (i.e. diabetes mellitus, diabetes insipidus, optic atrophy). Every patients will receive over three years a treatment by VPA (Depakine chrono).\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "my_result = sorted(range(len(count)), reverse = True,key=lambda k: count[k])[:10]\n",
    "print(my_result)\n",
    "for j in range (len(my_result)):\n",
    "    print(short_des[my_result[j]])\n",
    "    print(\"------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pricingmarketing']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "word_data = \"pricingmarketing\"\n",
    "nltk_tokens = nltk.word_tokenize(word_data)\n",
    "print (nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordninja in /Users/moweiting/Desktop/anaconda3/lib/python3.8/site-packages (2.0.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install wordninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pricing', 'marketing']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wordninja\n",
    "wordninja.split('pricingmarketing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## implement named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.8 from \"/Users/moweiting/Desktop/anaconda3/bin/python\"\n  * The NumPy version is: \"1.20.3\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: cannot import name '_set_madvise_hugepage' from 'numpy.core._multiarray_umath' (/Users/moweiting/Desktop/anaconda3/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-darwin.so)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/Desktop/anaconda3/lib/python3.8/site-packages/numpy/core/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmultiarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/anaconda3/lib/python3.8/site-packages/numpy/core/multiarray.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# _get_ndarray_c_version is semi-public, on purpose not added to __all__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m from ._multiarray_umath import (\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0m_fastCopyAndTranspose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_flagdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_insert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_vec_string\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_set_madvise_hugepage' from 'numpy.core._multiarray_umath' (/Users/moweiting/Desktop/anaconda3/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-darwin.so)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9c459f8c77b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# from spacy import displacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpprint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/anaconda3/lib/python3.8/site-packages/en_core_web_sm/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model_from_init_py\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_model_meta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/anaconda3/lib/python3.8/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# These are imported as part of the API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mthinc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneural\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprefer_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/anaconda3/lib/python3.8/site-packages/thinc/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Necessary for some side-effects in Cython. Not sure I understand.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mabout\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__version__\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/anaconda3/lib/python3.8/site-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/anaconda3/lib/python3.8/site-packages/numpy/core/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \"\"\" % (sys.version_info[0], sys.version_info[1], sys.executable,\n\u001b[1;32m     47\u001b[0m         __version__, exc)\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0menvkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menv_added\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.8 from \"/Users/moweiting/Desktop/anaconda3/bin/python\"\n  * The NumPy version is: \"1.20.3\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: cannot import name '_set_madvise_hugepage' from 'numpy.core._multiarray_umath' (/Users/moweiting/Desktop/anaconda3/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-darwin.so)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "from pprint import pprint\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.8 from \"/Users/moweiting/Desktop/anaconda3/bin/python\"\n  * The NumPy version is: \"1.20.3\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: cannot import name '_set_madvise_hugepage' from 'numpy.core._multiarray_umath' (/Users/moweiting/Desktop/anaconda3/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-darwin.so)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/Desktop/anaconda3/lib/python3.8/site-packages/numpy/core/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmultiarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/anaconda3/lib/python3.8/site-packages/numpy/core/multiarray.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# _get_ndarray_c_version is semi-public, on purpose not added to __all__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m from ._multiarray_umath import (\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0m_fastCopyAndTranspose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_flagdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_insert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_vec_string\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_set_madvise_hugepage' from 'numpy.core._multiarray_umath' (/Users/moweiting/Desktop/anaconda3/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-darwin.so)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ebc127b7749f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/anaconda3/lib/python3.8/site-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/anaconda3/lib/python3.8/site-packages/numpy/core/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \"\"\" % (sys.version_info[0], sys.version_info[1], sys.executable,\n\u001b[1;32m     47\u001b[0m         __version__, exc)\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0menvkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menv_added\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.8 from \"/Users/moweiting/Desktop/anaconda3/bin/python\"\n  * The NumPy version is: \"1.20.3\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: cannot import name '_set_madvise_hugepage' from 'numpy.core._multiarray_umath' (/Users/moweiting/Desktop/anaconda3/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-darwin.so)\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "print(numpy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Sarcopenia, 'B', 'GPE'),\n",
      " (can, 'O', ''),\n",
      " (occur, 'O', ''),\n",
      " (or, 'O', ''),\n",
      " (increase, 'O', ''),\n",
      " (due, 'O', ''),\n",
      " (to, 'O', ''),\n",
      " (sedentary, 'O', ''),\n",
      " (lifestyles, 'O', ''),\n",
      " (,, 'O', ''),\n",
      " (physical, 'O', ''),\n",
      " (inactivity, 'O', ''),\n",
      " (or, 'O', ''),\n",
      " (chronic, 'O', ''),\n",
      " (endocrine, 'O', ''),\n",
      " (and, 'O', ''),\n",
      " (inflammatory, 'O', ''),\n",
      " (disorders, 'O', ''),\n",
      " (,, 'O', ''),\n",
      " (this, 'O', ''),\n",
      " (pathology, 'O', ''),\n",
      " (is, 'O', ''),\n",
      " (much, 'O', ''),\n",
      " (more, 'O', ''),\n",
      " (frequent, 'O', ''),\n",
      " (in, 'O', ''),\n",
      " (older, 'O', ''),\n",
      " (people, 'O', ''),\n",
      " (due, 'O', ''),\n",
      " (to, 'O', ''),\n",
      " (the, 'O', ''),\n",
      " (added, 'O', ''),\n",
      " (risk, 'O', ''),\n",
      " (factors, 'O', ''),\n",
      " (and, 'O', ''),\n",
      " (the, 'O', ''),\n",
      " (fact, 'O', ''),\n",
      " (that, 'O', ''),\n",
      " (the, 'O', ''),\n",
      " (physiological, 'O', ''),\n",
      " (ageing, 'O', ''),\n",
      " (process, 'O', ''),\n",
      " (generates, 'O', ''),\n",
      " (a, 'O', ''),\n",
      " (pro, 'O', ''),\n",
      " (-, 'O', ''),\n",
      " (inflammatory, 'O', ''),\n",
      " (situation, 'O', ''),\n",
      " (and, 'O', ''),\n",
      " (an, 'O', ''),\n",
      " (alteration, 'O', ''),\n",
      " (in, 'O', ''),\n",
      " (the, 'O', ''),\n",
      " (synthesis, 'O', ''),\n",
      " (of, 'O', ''),\n",
      " (hormones, 'O', ''),\n",
      " (and, 'O', ''),\n",
      " (myokines, 'O', ''),\n",
      " (,, 'O', ''),\n",
      " (it, 'O', ''),\n",
      " (has, 'O', ''),\n",
      " (been, 'O', ''),\n",
      " (observed, 'O', ''),\n",
      " (that, 'O', ''),\n",
      " (the, 'O', ''),\n",
      " (loss, 'O', ''),\n",
      " (of, 'O', ''),\n",
      " (strength, 'O', ''),\n",
      " (causes, 'O', ''),\n",
      " (functional, 'O', ''),\n",
      " (deterioration, 'O', ''),\n",
      " (and, 'O', ''),\n",
      " (a, 'O', ''),\n",
      " (significant, 'O', ''),\n",
      " (increase, 'O', ''),\n",
      " (in, 'O', ''),\n",
      " (the, 'O', ''),\n",
      " (person, 'O', ''),\n",
      " ('s, 'O', ''),\n",
      " (dependence, 'O', ''),\n",
      " (,, 'O', ''),\n",
      " (reduces, 'O', ''),\n",
      " (their, 'O', ''),\n",
      " (functional, 'O', ''),\n",
      " (status, 'O', ''),\n",
      " (and, 'O', ''),\n",
      " (quality, 'O', ''),\n",
      " (of, 'O', ''),\n",
      " (life, 'O', ''),\n",
      " (,, 'O', ''),\n",
      " (and, 'O', ''),\n",
      " (may, 'O', ''),\n",
      " (increase, 'O', ''),\n",
      " (the, 'O', ''),\n",
      " (risk, 'O', ''),\n",
      " (of, 'O', ''),\n",
      " (falls, 'O', ''),\n",
      " (,, 'O', ''),\n",
      " (thereby, 'O', ''),\n",
      " (increasing, 'O', ''),\n",
      " (mortality, 'O', ''),\n",
      " (., 'O', ''),\n",
      " (\n",
      ", 'O', ''),\n",
      " (Blood, 'O', ''),\n",
      " (flow, 'O', ''),\n",
      " (restriction, 'O', ''),\n",
      " ((, 'O', ''),\n",
      " (BRR, 'B', 'ORG'),\n",
      " (), 'O', ''),\n",
      " (and, 'O', ''),\n",
      " (focal, 'O', ''),\n",
      " (vibration, 'O', ''),\n",
      " ((, 'O', ''),\n",
      " (FV, 'B', 'ORG'),\n",
      " (), 'O', ''),\n",
      " (,, 'O', ''),\n",
      " (which, 'O', ''),\n",
      " (aim, 'O', ''),\n",
      " (to, 'O', ''),\n",
      " (achieve, 'O', ''),\n",
      " (muscular, 'O', ''),\n",
      " (hypertrophy, 'O', ''),\n",
      " (without, 'O', ''),\n",
      " (the, 'O', ''),\n",
      " (need, 'O', ''),\n",
      " (to, 'O', ''),\n",
      " (use, 'O', ''),\n",
      " (high, 'O', ''),\n",
      " (loads, 'O', ''),\n",
      " (or, 'O', ''),\n",
      " (intensities, 'O', ''),\n",
      " (,, 'O', ''),\n",
      " (VF, 'O', ''),\n",
      " (or, 'O', ''),\n",
      " (BFR, 'O', ''),\n",
      " (brings, 'O', ''),\n",
      " (improvements, 'O', ''),\n",
      " (to, 'O', ''),\n",
      " (elderly, 'O', ''),\n",
      " (people, 'O', ''),\n",
      " (with, 'O', ''),\n",
      " (sarcopnoea, 'O', ''),\n",
      " (., 'O', ''),\n",
      " (\n",
      ", 'O', ''),\n",
      " (The, 'O', ''),\n",
      " (hipotesis, 'O', ''),\n",
      " (of, 'O', ''),\n",
      " (this, 'O', ''),\n",
      " (study, 'O', ''),\n",
      " (is, 'O', ''),\n",
      " (the, 'O', ''),\n",
      " (addition, 'O', ''),\n",
      " (of, 'O', ''),\n",
      " (BFR, 'O', ''),\n",
      " (or, 'O', ''),\n",
      " (VF, 'O', ''),\n",
      " (techniques, 'O', ''),\n",
      " (to, 'O', ''),\n",
      " (training, 'O', ''),\n",
      " (results, 'O', ''),\n",
      " (in, 'O', ''),\n",
      " (greater, 'O', ''),\n",
      " (improvements, 'O', ''),\n",
      " (in, 'O', ''),\n",
      " (circulating, 'O', ''),\n",
      " (myokine, 'O', ''),\n",
      " (concentrations, 'O', ''),\n",
      " (and, 'O', ''),\n",
      " (functional, 'O', ''),\n",
      " (tests, 'O', ''),\n",
      " (than, 'O', ''),\n",
      " (not, 'O', ''),\n",
      " (adding, 'O', ''),\n",
      " (it, 'O', ''),\n",
      " (., 'O', ''),\n",
      " (\n",
      ", 'O', ''),\n",
      " (This, 'O', ''),\n",
      " (study, 'O', ''),\n",
      " (has, 'O', ''),\n",
      " (the, 'O', ''),\n",
      " (objective, 'O', ''),\n",
      " (to, 'O', ''),\n",
      " (determinate, 'O', ''),\n",
      " (whether, 'O', ''),\n",
      " (biochemical, 'O', ''),\n",
      " (markers, 'O', ''),\n",
      " (in, 'O', ''),\n",
      " (serology, 'O', ''),\n",
      " (are, 'O', ''),\n",
      " (able, 'O', ''),\n",
      " (to, 'O', ''),\n",
      " (correlate, 'O', ''),\n",
      " (with, 'O', ''),\n",
      " (improvements, 'O', ''),\n",
      " (in, 'O', ''),\n",
      " (strength, 'O', ''),\n",
      " (,, 'O', ''),\n",
      " (also, 'O', ''),\n",
      " (to, 'O', ''),\n",
      " (study, 'O', ''),\n",
      " (whether, 'O', ''),\n",
      " (the, 'O', ''),\n",
      " (plasma, 'O', ''),\n",
      " (levels, 'O', ''),\n",
      " (of, 'O', ''),\n",
      " (apelin, 'O', ''),\n",
      " (,, 'O', ''),\n",
      " (myomyostatin, 'O', ''),\n",
      " (and, 'O', ''),\n",
      " (lL6, 'O', ''),\n",
      " (are, 'O', ''),\n",
      " (modified, 'O', ''),\n",
      " (with, 'O', ''),\n",
      " (entraining, 'O', ''),\n",
      " (,, 'O', ''),\n",
      " (to, 'O', ''),\n",
      " (determine, 'O', ''),\n",
      " (whether, 'O', ''),\n",
      " (plasma, 'O', ''),\n",
      " (levels, 'O', ''),\n",
      " (of, 'O', ''),\n",
      " (apelin, 'O', ''),\n",
      " (,, 'O', ''),\n",
      " (myomyostatin, 'O', ''),\n",
      " (and, 'O', ''),\n",
      " (lL6, 'O', ''),\n",
      " (are, 'O', ''),\n",
      " (further, 'O', ''),\n",
      " (increased, 'O', ''),\n",
      " (by, 'O', ''),\n",
      " (training, 'O', ''),\n",
      " (associated, 'O', ''),\n",
      " (with, 'O', ''),\n",
      " (VF, 'O', ''),\n",
      " (and/or, 'O', ''),\n",
      " (BFR, 'O', ''),\n",
      " (and, 'O', ''),\n",
      " (evaluate, 'O', ''),\n",
      " (the, 'O', ''),\n",
      " (effectiveness, 'O', ''),\n",
      " (of, 'O', ''),\n",
      " (different, 'O', ''),\n",
      " (interventions, 'O', ''),\n",
      " (in, 'O', ''),\n",
      " (improving, 'O', ''),\n",
      " (functional, 'O', ''),\n",
      " (tests, 'O', ''),\n",
      " (., 'O', ''),\n",
      " (\n",
      ", 'O', ''),\n",
      " (The, 'O', ''),\n",
      " (methodology, 'O', ''),\n",
      " (of, 'O', ''),\n",
      " (the, 'O', ''),\n",
      " (study, 'O', ''),\n",
      " (is, 'O', ''),\n",
      " (a, 'O', ''),\n",
      " (single, 'O', ''),\n",
      " (-, 'O', ''),\n",
      " (blind, 'O', ''),\n",
      " (,, 'O', ''),\n",
      " (randomised, 'O', ''),\n",
      " (,, 'O', ''),\n",
      " (clinical, 'O', ''),\n",
      " (trial, 'O', ''),\n",
      " (will, 'O', ''),\n",
      " (be, 'O', ''),\n",
      " (conducted, 'O', ''),\n",
      " (., 'O', ''),\n",
      " (The, 'O', ''),\n",
      " (study, 'O', ''),\n",
      " (population, 'O', ''),\n",
      " (is, 'O', ''),\n",
      " (people, 'O', ''),\n",
      " (over, 'B', 'DATE'),\n",
      " (65, 'I', 'DATE'),\n",
      " (years, 'I', 'DATE'),\n",
      " (of, 'I', 'DATE'),\n",
      " (age, 'I', 'DATE'),\n",
      " (,, 'O', ''),\n",
      " (sedentary, 'O', ''),\n",
      " (,, 'O', ''),\n",
      " (with, 'O', ''),\n",
      " (functional, 'O', ''),\n",
      " (independence, 'O', ''),\n",
      " (and, 'O', ''),\n",
      " (with, 'O', ''),\n",
      " (a, 'O', ''),\n",
      " (state, 'O', ''),\n",
      " (of, 'O', ''),\n",
      " (health, 'O', ''),\n",
      " (that, 'O', ''),\n",
      " (allows, 'O', ''),\n",
      " (them, 'O', ''),\n",
      " (to, 'O', ''),\n",
      " (carry, 'O', ''),\n",
      " (out, 'O', ''),\n",
      " (physical, 'O', ''),\n",
      " (activity, 'O', ''),\n",
      " (., 'O', ''),\n",
      " (The, 'O', ''),\n",
      " (study, 'O', ''),\n",
      " (is, 'O', ''),\n",
      " (planned, 'O', ''),\n",
      " (as, 'O', ''),\n",
      " (a, 'O', ''),\n",
      " (pilot, 'O', ''),\n",
      " (study, 'O', ''),\n",
      " (and, 'O', ''),\n",
      " (will, 'O', ''),\n",
      " (consist, 'O', ''),\n",
      " (of, 'O', ''),\n",
      " (30, 'B', 'CARDINAL'),\n",
      " (subjects, 'O', ''),\n",
      " (distributed, 'O', ''),\n",
      " (in, 'O', ''),\n",
      " (:, 'O', ''),\n",
      " (10, 'B', 'CARDINAL'),\n",
      " (people, 'O', ''),\n",
      " (in, 'O', ''),\n",
      " (the, 'O', ''),\n",
      " (control, 'O', ''),\n",
      " (group, 'O', ''),\n",
      " ((, 'O', ''),\n",
      " (CG, 'O', ''),\n",
      " (), 'O', ''),\n",
      " (,, 'O', ''),\n",
      " (10, 'B', 'CARDINAL'),\n",
      " (in, 'O', ''),\n",
      " (the, 'O', ''),\n",
      " (experimental, 'O', ''),\n",
      " (vibration, 'O', ''),\n",
      " (group, 'O', ''),\n",
      " ((, 'O', ''),\n",
      " (GE, 'B', 'ORG'),\n",
      " (-, 'O', ''),\n",
      " (V, 'O', ''),\n",
      " (), 'O', ''),\n",
      " (and, 'O', ''),\n",
      " (10, 'B', 'CARDINAL'),\n",
      " (in, 'O', ''),\n",
      " (the, 'O', ''),\n",
      " (experimental, 'O', ''),\n",
      " (group, 'O', ''),\n",
      " (with, 'O', ''),\n",
      " (restriction, 'O', ''),\n",
      " ((, 'O', ''),\n",
      " (GE, 'B', 'ORG'),\n",
      " (-, 'O', ''),\n",
      " (R, 'O', ''),\n",
      " (), 'O', ''),\n",
      " (., 'O', ''),\n",
      " (\n",
      ", 'O', ''),\n",
      " (The, 'O', ''),\n",
      " (variables, 'O', ''),\n",
      " (to, 'O', ''),\n",
      " (be, 'O', ''),\n",
      " (measured, 'O', ''),\n",
      " (are, 'O', ''),\n",
      " (anthropometric, 'O', ''),\n",
      " (variables, 'O', ''),\n",
      " (,, 'O', ''),\n",
      " (biochemical, 'O', ''),\n",
      " (markers, 'O', ''),\n",
      " (,, 'O', ''),\n",
      " (variables, 'O', ''),\n",
      " (of, 'O', ''),\n",
      " (neuromuscular, 'O', ''),\n",
      " (function, 'O', ''),\n",
      " (,, 'O', ''),\n",
      " (information, 'O', ''),\n",
      " (about, 'O', ''),\n",
      " (fragility, 'O', ''),\n",
      " (and, 'O', ''),\n",
      " (independence, 'O', ''),\n",
      " (,, 'O', ''),\n",
      " (an, 'O', ''),\n",
      " (functionality, 'O', ''),\n",
      " (., 'O', ''),\n",
      " (\n",
      ", 'O', ''),\n",
      " (The, 'O', ''),\n",
      " (intervention, 'O', ''),\n",
      " (will, 'O', ''),\n",
      " (be, 'O', ''),\n",
      " (a, 'O', ''),\n",
      " (training, 'O', ''),\n",
      " (in, 'O', ''),\n",
      " (the, 'O', ''),\n",
      " (control, 'O', ''),\n",
      " (group, 'O', ''),\n",
      " (,, 'O', ''),\n",
      " (the, 'O', ''),\n",
      " (FV, 'B', 'ORG'),\n",
      " (and, 'O', ''),\n",
      " (BFR, 'O', ''),\n",
      " (groups, 'O', ''),\n",
      " (will, 'O', ''),\n",
      " (be, 'O', ''),\n",
      " (3, 'B', 'CARDINAL'),\n",
      " (times, 'O', ''),\n",
      " (a, 'O', ''),\n",
      " (week, 'O', ''),\n",
      " (,, 'O', ''),\n",
      " (with, 'O', ''),\n",
      " (a, 'O', ''),\n",
      " (warm, 'O', ''),\n",
      " (-, 'O', ''),\n",
      " (up, 'O', ''),\n",
      " (,, 'O', ''),\n",
      " (a, 'O', ''),\n",
      " (main, 'O', ''),\n",
      " (block, 'O', ''),\n",
      " (with, 'O', ''),\n",
      " (aerobic, 'O', ''),\n",
      " (work, 'O', ''),\n",
      " (,, 'O', ''),\n",
      " (strength, 'O', ''),\n",
      " (work, 'O', ''),\n",
      " (and, 'O', ''),\n",
      " (training, 'O', ''),\n",
      " (and, 'O', ''),\n",
      " (coordination, 'O', ''),\n",
      " (work, 'O', ''),\n",
      " (,, 'O', ''),\n",
      " (and, 'O', ''),\n",
      " (finally, 'O', ''),\n",
      " (a, 'O', ''),\n",
      " (return, 'O', ''),\n",
      " (to, 'O', ''),\n",
      " (calm, 'O', ''),\n",
      " (,, 'O', ''),\n",
      " (in, 'O', ''),\n",
      " (the, 'O', ''),\n",
      " (experimental, 'O', ''),\n",
      " (groups, 'O', ''),\n",
      " (the, 'O', ''),\n",
      " (strength, 'O', ''),\n",
      " (work, 'O', ''),\n",
      " (will, 'O', ''),\n",
      " (be, 'O', ''),\n",
      " (carried, 'O', ''),\n",
      " (out, 'O', ''),\n",
      " (with, 'O', ''),\n",
      " (these, 'O', ''),\n",
      " (instruments, 'O', ''),\n",
      " (., 'O', '')]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(short_des[1])\n",
    "pprint([(X,X.ent_iob_, X.ent_type_) for X in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sarcopenia', 'GPE'),\n",
      " ('BRR', 'ORG'),\n",
      " ('FV', 'ORG'),\n",
      " ('over 65 years of age', 'DATE'),\n",
      " ('30', 'CARDINAL'),\n",
      " ('10', 'CARDINAL'),\n",
      " ('10', 'CARDINAL'),\n",
      " ('GE', 'ORG'),\n",
      " ('10', 'CARDINAL'),\n",
      " ('GE', 'ORG'),\n",
      " ('FV', 'ORG'),\n",
      " ('3', 'CARDINAL')]\n"
     ]
    }
   ],
   "source": [
    "pprint([(X.text, X.label_) for X in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1d451af19e75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshort_des\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshort_des\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnew_ner\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_ner\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mNER_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "NER_text = []\n",
    "NER_pair = {}\n",
    "\n",
    "for i in range (0, len(short_des)):\n",
    "    doc = nlp(short_des[i])\n",
    "    for new_ner in ([X.text for X in doc.ents]):\n",
    "        if new_ner not in NER_text:\n",
    "            NER_text.append(new_ner)\n",
    "            NER_pair.append({new_ner: i})\n",
    "        else:\n",
    "            if not isinstance(NER_pair[new_ner], list):\n",
    "                NER_pair[new_ner] = [NER_pair[new_ner]]\n",
    "            dict_obj[key].append(i)\n",
    "print(NER_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add in new text \"new_doc\"\n",
    "## suppose my new_doc is short_des[1]\n",
    "new_doc = short_des[1]\n",
    "new_doc = nlp(new_doc)\n",
    "for new_ner in ([X.text for X in new_doc.ents]):\n",
    "    if new_ner not in NER_text:\n",
    "        NER_text.append(new_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2582\n"
     ]
    }
   ],
   "source": [
    "labels = [x.label_ for x in doc.ents]\n",
    "Counter(labels)\n",
    "print(len(NER_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import jaccard_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_jaccard(t1,t2):\n",
    "    \n",
    "    intersect = [value for value in t1 if value in t2] \n",
    "    \n",
    "    union=[]\n",
    "    union.extend(t1)\n",
    "    union.extend(t2)\n",
    "    union=list(set(union))\n",
    "    \n",
    "    \n",
    "    jaccard=(len(intersect))/(len(union)+0.01)\n",
    "    return jaccard\n",
    "\n",
    "phrases = NER_text\n",
    "query = \"international trade\"\n",
    "results_jaccard=[]\n",
    "results_glove = []\n",
    "\n",
    "\n",
    "for p in phrases:\n",
    "\n",
    "    tokens_1=[t for t in p.split() if t in model.words]\n",
    "    tokens_2=[t for t in query.split() if t in model.words]\n",
    "    \n",
    "    \n",
    "    jaccard=compute_jaccard(p,query)\n",
    "    results_jaccard.append([p,jaccard])\n",
    "\n",
    "    cosine=0\n",
    "    if (len(tokens_1) > 0 and len(tokens_2)>0):\n",
    "        vector1 = np.array(model.get_word_vector(p))\n",
    "        vector2 = np.array(model.get_word_vector(query))\n",
    "        cosine = np.dot(vector1,vector2)/(np.linalg.norm(vector1)*np.linalg.norm(vector2))\n",
    "        results_glove.append([p,cosine])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrases most similar to 'international trade' using glove word embeddings\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>the Association for the Advancement of Medical...</td>\n",
       "      <td>0.763784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>the World Health Organization Regional Office ...</td>\n",
       "      <td>0.718162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>International Classification of Diseases</td>\n",
       "      <td>0.717460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>the World Health Organization</td>\n",
       "      <td>0.689953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>the association of at least 2</td>\n",
       "      <td>0.675008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>4.5&amp; of SV LSC</td>\n",
       "      <td>-0.009109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>2,500 to 3,000</td>\n",
       "      <td>-0.014083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>4.5 mm</td>\n",
       "      <td>-0.023539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>PD of DSP107</td>\n",
       "      <td>-0.086495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>1 to 10</td>\n",
       "      <td>-0.116370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>771 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                phrase     score\n",
       "576  the Association for the Advancement of Medical...  0.763784\n",
       "664  the World Health Organization Regional Office ...  0.718162\n",
       "685           International Classification of Diseases  0.717460\n",
       "317                      the World Health Organization  0.689953\n",
       "204                      the association of at least 2  0.675008\n",
       "..                                                 ...       ...\n",
       "390                                     4.5& of SV LSC -0.009109\n",
       "549                                     2,500 to 3,000 -0.014083\n",
       "661                                             4.5 mm -0.023539\n",
       "244                                       PD of DSP107 -0.086495\n",
       "472                                            1 to 10 -0.116370\n",
       "\n",
       "[771 rows x 2 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Phrases most similar to '{0}' using glove word embeddings\".format(query))\n",
    "pd.DataFrame(results_glove,columns=[\"phrase\",\"score\"]).sort_values(by=[\"score\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrases most similar to 'international trade' using jaccard similarity\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1927</th>\n",
       "      <td>the Association for the Advancement of Medical...</td>\n",
       "      <td>3.498834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727</th>\n",
       "      <td>The Unified Protocol for Transdiagnostic Treat...</td>\n",
       "      <td>2.862335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>Interventional Radiology Liver Directed Therap...</td>\n",
       "      <td>2.534809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2315</th>\n",
       "      <td>Safety and Efficacy of Different PANZYGA Dose ...</td>\n",
       "      <td>2.436739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1549</th>\n",
       "      <td>the Tracking Response to Antidepressants in Ad...</td>\n",
       "      <td>2.407996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572</th>\n",
       "      <td>DBS-PS</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567</th>\n",
       "      <td>STN-DBS</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>SABR</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>MM</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1639</th>\n",
       "      <td>62%</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2582 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 phrase     score\n",
       "1927  the Association for the Advancement of Medical...  3.498834\n",
       "1727  The Unified Protocol for Transdiagnostic Treat...  2.862335\n",
       "1582  Interventional Radiology Liver Directed Therap...  2.534809\n",
       "2315  Safety and Efficacy of Different PANZYGA Dose ...  2.436739\n",
       "1549  the Tracking Response to Antidepressants in Ad...  2.407996\n",
       "...                                                 ...       ...\n",
       "1572                                             DBS-PS  0.000000\n",
       "1567                                            STN-DBS  0.000000\n",
       "382                                                SABR  0.000000\n",
       "1563                                                 MM  0.000000\n",
       "1639                                                62%  0.000000\n",
       "\n",
       "[2582 rows x 2 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Phrases most similar to '{0}' using jaccard similarity\".format(query))\n",
    "pd.DataFrame(results_jaccard,columns=[\"phrase\",\"score\"]).sort_values(by=[\"score\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the most related study\n",
    "results = list()\n",
    "for i in range (0,10):\n",
    "    my_word = result_glove[i][0]\n",
    "    my_study = NER_pair[my_word]\n",
    "    for ele in my_study:\n",
    "        if not in result:\n",
    "            result.append(ele)\n",
    "for ele in result:\n",
    "    print(short_des[ele])\n",
    "    print(\"-------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
